{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Training YOLOv8 Model\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZSBayPv08uA",
        "outputId": "2f6916ef-bda4-4666-ac44-1bb1eaeebd2a"
      },
      "outputs": [],
      "source": [
        "# If you use Colab, this block needed for connect to your drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parent directory where files will be cloned\n",
        "MAIN_DIRECTORY = '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUeNFVaYqWzG",
        "outputId": "a8208a31-be23-4d34-daaa-3849055e51d6"
      },
      "outputs": [],
      "source": [
        "%cd {MAIN_DIRECTORY}\n",
        "!git clone https://github.com/ultralytics/ultralytics\n",
        "%pip install -qe ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "ac231007-6077-4583-c83b-47c8b25b403c"
      },
      "outputs": [],
      "source": [
        "# If you get error on pip install step above, run this block.\n",
        "# Pip install method (recommended)\n",
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This needed for online augmentations\n",
        "%pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ymC-j-S4i-Py"
      },
      "outputs": [],
      "source": [
        "# You can use writetemplate and writefile functions for modify on files or crate files quickly\n",
        "\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))\n",
        "\n",
        "def writefile(self, line, cell):\n",
        "    # docs & arg parsing\n",
        "    with io.open(filename, mode, encoding='utf-8') as f:\n",
        "        f.write(cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usOumbPxrt2t"
      },
      "outputs": [],
      "source": [
        "# You must create a .yaml file that included dataset infos\n",
        "%%writetemplate {MAIN_DIRECTORY}/ultralytics/ultralytics/yolo/data/datasets/nut_bolt.yaml\n",
        "\n",
        "\n",
        "path: {MAIN_DIRECTORY}/yololabels_no_track_id\n",
        "train: train  # train images  \n",
        "val: val # val images\n",
        "test:  # test images (optional)\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: Bolt\n",
        "  1: Nut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou_Aowf1ixa4"
      },
      "outputs": [],
      "source": [
        "# You must change default.yaml or create new .yaml file for editing hyperparameters. \n",
        "\n",
        "%%writetemplate {MAIN_DIRECTORY}/ultralytics/ultralytics/yolo/cfg/default.yaml\n",
        "\n",
        "task: detect  # inference task, i.e. detect, segment, classify\n",
        "mode: train  # YOLO mode, i.e. train, val, predict, export\n",
        "\n",
        "# Train settings -------------------------------------------------------------------------------------------------------\n",
        "model: yolov8s.pt # path to model file, i.e. yolov8n.pt, yolov8n.yaml\n",
        "data: {MAIN_DIRECTORY}/ultralytics/ultralytics/yolo/data/datasets/nut_bolt.yaml # path to data file, i.e. i.e. coco128.yaml\n",
        "epochs: 100  # number of epochs to train for\n",
        "patience: 15 #50  # epochs to wait for no observable improvement for early stopping of training\n",
        "batch: 32 #64  # number of images per batch (-1 for AutoBatch)\n",
        "imgsz: 640  # size of input images as integer or w,h\n",
        "save: True  # save train checkpoints and predict results\n",
        "cache: False  # True/ram, disk or False. Use cache for data loading\n",
        "device:  # device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu\n",
        "workers: 8  # number of worker threads for data loading (per RANK if DDP)\n",
        "project:  # project name\n",
        "name:  # experiment name\n",
        "exist_ok: False  # whether to overwrite existing experiment\n",
        "pretrained: False  # whether to use a pretrained model\n",
        "optimizer: SGD  # optimizer to use, choices=['SGD', 'Adam', 'AdamW', 'RMSProp']\n",
        "verbose: True  # whether to print verbose output\n",
        "seed: 0  # random seed for reproducibility\n",
        "deterministic: True  # whether to enable deterministic mode\n",
        "single_cls: False  # train multi-class data as single-class\n",
        "image_weights: False  # use weighted image selection for training\n",
        "rect: False  # support rectangular training\n",
        "cos_lr: False  # use cosine learning rate scheduler\n",
        "close_mosaic: 10  # disable mosaic augmentation for final 10 epochs\n",
        "resume: False  # resume training from last checkpoint\n",
        "# Segmentation\n",
        "overlap_mask: True  # masks should overlap during training (segment train only)\n",
        "mask_ratio: 4  # mask downsample ratio (segment train only)\n",
        "# Classification\n",
        "dropout: 0.0  # use dropout regularization (classify train only)\n",
        "\n",
        "# Val/Test settings ----------------------------------------------------------------------------------------------------\n",
        "val: True  # validate/test during training\n",
        "save_json: False  # save results to JSON file\n",
        "save_hybrid: False  # save hybrid version of labels (labels + additional predictions)\n",
        "conf:  # object confidence threshold for detection (default 0.25 predict, 0.001 val)\n",
        "iou: 0.7  # intersection over union (IoU) threshold for NMS\n",
        "max_det: 300  # maximum number of detections per image\n",
        "half: False  # use half precision (FP16)\n",
        "dnn: False  # use OpenCV DNN for ONNX inference\n",
        "plots: True  # save plots during train/val\n",
        "\n",
        "# Prediction settings --------------------------------------------------------------------------------------------------\n",
        "source:  # source directory for images or videos\n",
        "show: False  # show results if possible\n",
        "save_txt: False  # save results as .txt file\n",
        "save_conf: False  # save results with confidence scores\n",
        "save_crop: False  # save cropped images with results\n",
        "hide_labels: False  # hide labels\n",
        "hide_conf: False  # hide confidence scores\n",
        "vid_stride: 1  # video frame-rate stride\n",
        "line_thickness: 3  # bounding box thickness (pixels)\n",
        "visualize: False  # visualize model features\n",
        "augment: False  # apply image augmentation to prediction sources\n",
        "agnostic_nms: False  # class-agnostic NMS\n",
        "classes:  # filter results by class, i.e. class=0, or class=[0,2,3]\n",
        "retina_masks: False  # use high-resolution segmentation masks\n",
        "boxes: True # Show boxes in segmentation predictions\n",
        "\n",
        "# Export settings ------------------------------------------------------------------------------------------------------\n",
        "format: torchscript  # format to export to\n",
        "keras: False  # use Keras\n",
        "optimize: False  # TorchScript: optimize for mobile\n",
        "int8: False  # CoreML/TF INT8 quantization\n",
        "dynamic: False  # ONNX/TF/TensorRT: dynamic axes\n",
        "simplify: False  # ONNX: simplify model\n",
        "opset:  # ONNX: opset version (optional)\n",
        "workspace: 4  # TensorRT: workspace size (GB)\n",
        "nms: False  # CoreML: add NMS\n",
        "\n",
        "# Hyperparameters ------------------------------------------------------------------------------------------------------\n",
        "lr0: 0.01 #0.005#0.01  # initial learning rate (i.e. SGD=1E-2, Adam=1E-3)\n",
        "lrf: 0.01 #0.01 # final learning rate (lr0 * lrf)\n",
        "momentum: 0.937  # SGD momentum/Adam beta1\n",
        "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
        "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
        "warmup_momentum: 0.8  # warmup initial momentum\n",
        "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
        "box: 7.5  # box loss gain\n",
        "cls: 0.5  # cls loss gain (scale with pixels)\n",
        "dfl: 1.5  # dfl loss gain\n",
        "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
        "label_smoothing: 0.0  # label smoothing (fraction)\n",
        "nbs: 64  # nominal batch size\n",
        "hsv_h: 0.015 # 0.007  # image HSV-Hue augmentation (fraction)\n",
        "hsv_s: 0.7  #0.035 # image HSV-Saturation augmentation (fraction)#doygunluk\n",
        "hsv_v: 0.4  #0.2 # image HSV-Value augmentation (fraction)#parlaklik\n",
        "degrees: 0.5 #0  # image rotation (+/- deg)\n",
        "translate: 0.1  # image translation (+/- fraction)\n",
        "scale: 0.5  # image scale (+/- gain)\n",
        "shear: 0.0  # image shear (+/- deg)\n",
        "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
        "flipud: 0.4  #0.0 # image flip up-down (probability)\n",
        "fliplr: 0.4  # image flip left-right (probability)\n",
        "mosaic: 1.0  # image mosaic (probability)\n",
        "mixup: 0.0  # image mixup (probability)\n",
        "copy_paste: 0.0  # segment copy-paste (probability)\n",
        "\n",
        "# Custom config.yaml ---------------------------------------------------------------------------------------------------\n",
        "cfg:  # for overriding defaults.yaml\n",
        "\n",
        "# Debug, do not modify -------------------------------------------------------------------------------------------------\n",
        "v5loader: False  # use legacy YOLOv5 dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bv2yrWUmr6h"
      },
      "outputs": [],
      "source": [
        "# If you needed edit model structure, you can do it in here.\n",
        "# You must pay attention chosen model path.\n",
        "\n",
        "%%writetemplate {MAIN_DIRECTORY}/ultralytics/ultralytics/models/v8/yolov8n.yaml\n",
        "\n",
        "\n",
        "nc: 2  # number of classes\n",
        "depth_multiple: 0.33  # scales module repeats\n",
        "width_multiple: 0.25  # scales convolution channels\n",
        "\n",
        "# YOLOv8.0n backbone\n",
        "backbone:\n",
        "  # [from, repeats, module, args]\n",
        "  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n",
        "  - [-1, 3, C2f, [128, True]]\n",
        "  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n",
        "  - [-1, 6, C2f, [256, True]]\n",
        "  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n",
        "  - [-1, 6, C2f, [512, True]]\n",
        "  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32\n",
        "  - [-1, 3, C2f, [1024, True]]\n",
        "  - [-1, 1, SPPF, [1024, 5]]  # 9\n",
        "\n",
        "# YOLOv8.0n head\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
        "  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n",
        "  - [-1, 3, C2f, [512]]  # 13\n",
        "\n",
        "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
        "  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n",
        "  - [-1, 3, C2f, [256]]  # 17 (P3/8-small)\n",
        "\n",
        "  - [-1, 1, Conv, [256, 3, 2]]\n",
        "  - [[-1, 12], 1, Concat, [1]]  # cat head P4\n",
        "  - [-1, 3, C2f, [512]]  # 20 (P4/16-medium)\n",
        "\n",
        "  - [-1, 1, Conv, [512, 3, 2]]\n",
        "  - [[-1, 9], 1, Concat, [1]]  # cat head P5\n",
        "  - [-1, 3, C2f, [1024]]  # 23 (P5/32-large)\n",
        "\n",
        "  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_sZTrj5prg3",
        "outputId": "3879c43d-4a20-4e23-bde6-c6b25cf9a8b4"
      },
      "outputs": [],
      "source": [
        "# Online augmentation parameters was marked below in class Albumentations in file \n",
        "\n",
        "%%writefile {MAIN_DIRECTORY}/ultralytics/ultralytics/yolo/data/augment.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from ..utils import LOGGER, colorstr\n",
        "from ..utils.checks import check_version\n",
        "from ..utils.instance import Instances\n",
        "from ..utils.metrics import bbox_ioa\n",
        "from ..utils.ops import segment2box\n",
        "from .utils import IMAGENET_MEAN, IMAGENET_STD, polygons2masks, polygons2masks_overlap\n",
        "\n",
        "\n",
        "# TODO: we might need a BaseTransform to make all these augments be compatible with both classification and semantic\n",
        "class BaseTransform:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def apply_image(self, labels):\n",
        "        pass\n",
        "\n",
        "    def apply_instances(self, labels):\n",
        "        pass\n",
        "\n",
        "    def apply_semantic(self, labels):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        self.apply_image(labels)\n",
        "        self.apply_instances(labels)\n",
        "        self.apply_semantic(labels)\n",
        "\n",
        "\n",
        "class Compose:\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, data):\n",
        "        for t in self.transforms:\n",
        "            data = t(data)\n",
        "        return data\n",
        "\n",
        "    def append(self, transform):\n",
        "        self.transforms.append(transform)\n",
        "\n",
        "    def tolist(self):\n",
        "        return self.transforms\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = f\"{self.__class__.__name__}(\"\n",
        "        for t in self.transforms:\n",
        "            format_string += \"\\n\"\n",
        "            format_string += f\"    {t}\"\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class BaseMixTransform:\n",
        "    \"\"\"This implementation is from mmyolo\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, pre_transform=None, p=0.0) -> None:\n",
        "        self.dataset = dataset\n",
        "        self.pre_transform = pre_transform\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        if random.uniform(0, 1) > self.p:\n",
        "            return labels\n",
        "\n",
        "        # get index of one or three other images\n",
        "        indexes = self.get_indexes()\n",
        "        if isinstance(indexes, int):\n",
        "            indexes = [indexes]\n",
        "\n",
        "        # get images information will be used for Mosaic or MixUp\n",
        "        mix_labels = [self.dataset.get_label_info(i) for i in indexes]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            for i, data in enumerate(mix_labels):\n",
        "                mix_labels[i] = self.pre_transform(data)\n",
        "        labels[\"mix_labels\"] = mix_labels\n",
        "\n",
        "        # Mosaic or MixUp\n",
        "        labels = self._mix_transform(labels)\n",
        "        labels.pop(\"mix_labels\", None)\n",
        "        return labels\n",
        "\n",
        "    def _mix_transform(self, labels):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_indexes(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Mosaic(BaseMixTransform):\n",
        "    \"\"\"Mosaic augmentation.\n",
        "    Args:\n",
        "        imgsz (Sequence[int]): Image size after mosaic pipeline of single\n",
        "            image. The shape order should be (height, width).\n",
        "            Default to (640, 640).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, imgsz=640, p=1.0, border=(0, 0)):\n",
        "        assert 0 <= p <= 1.0, \"The probability should be in range [0, 1]. \" f\"got {p}.\"\n",
        "        super().__init__(dataset=dataset, p=p)\n",
        "        self.dataset = dataset\n",
        "        self.imgsz = imgsz\n",
        "        self.border = border\n",
        "\n",
        "    def get_indexes(self):\n",
        "        return [random.randint(0, len(self.dataset) - 1) for _ in range(3)]\n",
        "\n",
        "    def _mix_transform(self, labels):\n",
        "        mosaic_labels = []\n",
        "        assert labels.get(\"rect_shape\", None) is None, \"rect and mosaic is exclusive.\"\n",
        "        assert len(labels.get(\"mix_labels\", [])) > 0, \"There are no other images for mosaic augment.\"\n",
        "        s = self.imgsz\n",
        "        yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.border)  # mosaic center x, y\n",
        "        for i in range(4):\n",
        "            labels_patch = (labels if i == 0 else labels[\"mix_labels\"][i - 1]).copy()\n",
        "            # Load image\n",
        "            img = labels_patch[\"img\"]\n",
        "            h, w = labels_patch.pop(\"resized_shape\")\n",
        "\n",
        "            # place img in img4\n",
        "            if i == 0:  # top left\n",
        "                img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
        "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
        "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
        "            elif i == 1:  # top right\n",
        "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
        "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
        "            elif i == 2:  # bottom left\n",
        "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
        "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n",
        "            elif i == 3:  # bottom right\n",
        "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
        "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
        "\n",
        "            img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
        "            padw = x1a - x1b\n",
        "            padh = y1a - y1b\n",
        "\n",
        "            labels_patch = self._update_labels(labels_patch, padw, padh)\n",
        "            mosaic_labels.append(labels_patch)\n",
        "        final_labels = self._cat_labels(mosaic_labels)\n",
        "        final_labels[\"img\"] = img4\n",
        "        return final_labels\n",
        "\n",
        "    def _update_labels(self, labels, padw, padh):\n",
        "        \"\"\"Update labels\"\"\"\n",
        "        nh, nw = labels[\"img\"].shape[:2]\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(nw, nh)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "    def _cat_labels(self, mosaic_labels):\n",
        "        if len(mosaic_labels) == 0:\n",
        "            return {}\n",
        "        cls = []\n",
        "        instances = []\n",
        "        for labels in mosaic_labels:\n",
        "            cls.append(labels[\"cls\"])\n",
        "            instances.append(labels[\"instances\"])\n",
        "        final_labels = {\n",
        "            \"im_file\": mosaic_labels[0][\"im_file\"],\n",
        "            \"ori_shape\": mosaic_labels[0][\"ori_shape\"],\n",
        "            \"resized_shape\": (self.imgsz * 2, self.imgsz * 2),\n",
        "            \"cls\": np.concatenate(cls, 0),\n",
        "            \"instances\": Instances.concatenate(instances, axis=0),\n",
        "            \"mosaic_border\": self.border}\n",
        "        final_labels[\"instances\"].clip(self.imgsz * 2, self.imgsz * 2)\n",
        "        return final_labels\n",
        "\n",
        "\n",
        "class MixUp(BaseMixTransform):\n",
        "\n",
        "    def __init__(self, dataset, pre_transform=None, p=0.0) -> None:\n",
        "        super().__init__(dataset=dataset, pre_transform=pre_transform, p=p)\n",
        "\n",
        "    def get_indexes(self):\n",
        "        return random.randint(0, len(self.dataset) - 1)\n",
        "\n",
        "    def _mix_transform(self, labels):\n",
        "        # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
        "        r = np.random.beta(32.0, 32.0)  # mixup ratio, alpha=beta=32.0\n",
        "        labels2 = labels[\"mix_labels\"][0]\n",
        "        labels[\"img\"] = (labels[\"img\"] * r + labels2[\"img\"] * (1 - r)).astype(np.uint8)\n",
        "        labels[\"instances\"] = Instances.concatenate([labels[\"instances\"], labels2[\"instances\"]], axis=0)\n",
        "        labels[\"cls\"] = np.concatenate([labels[\"cls\"], labels2[\"cls\"]], 0)\n",
        "        return labels\n",
        "\n",
        "\n",
        "class RandomPerspective:\n",
        "\n",
        "    def __init__(self,\n",
        "                 degrees=0.0,\n",
        "                 translate=0.1,\n",
        "                 scale=0.5,\n",
        "                 shear=0.0,\n",
        "                 perspective=0.0,\n",
        "                 border=(0, 0),\n",
        "                 pre_transform=None):\n",
        "        self.degrees = degrees\n",
        "        self.translate = translate\n",
        "        self.scale = scale\n",
        "        self.shear = shear\n",
        "        self.perspective = perspective\n",
        "        # mosaic border\n",
        "        self.border = border\n",
        "        self.pre_transform = pre_transform\n",
        "\n",
        "    def affine_transform(self, img, border):\n",
        "        # Center\n",
        "        C = np.eye(3)\n",
        "\n",
        "        C[0, 2] = -img.shape[1] / 2  # x translation (pixels)\n",
        "        C[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n",
        "\n",
        "        # Perspective\n",
        "        P = np.eye(3)\n",
        "        P[2, 0] = random.uniform(-self.perspective, self.perspective)  # x perspective (about y)\n",
        "        P[2, 1] = random.uniform(-self.perspective, self.perspective)  # y perspective (about x)\n",
        "\n",
        "        # Rotation and Scale\n",
        "        R = np.eye(3)\n",
        "        a = random.uniform(-self.degrees, self.degrees)\n",
        "        # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n",
        "        s = random.uniform(1 - self.scale, 1 + self.scale)\n",
        "        # s = 2 ** random.uniform(-scale, scale)\n",
        "        R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "\n",
        "        # Shear\n",
        "        S = np.eye(3)\n",
        "        S[0, 1] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # x shear (deg)\n",
        "        S[1, 0] = math.tan(random.uniform(-self.shear, self.shear) * math.pi / 180)  # y shear (deg)\n",
        "\n",
        "        # Translation\n",
        "        T = np.eye(3)\n",
        "        T[0, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[0]  # x translation (pixels)\n",
        "        T[1, 2] = random.uniform(0.5 - self.translate, 0.5 + self.translate) * self.size[1]  # y translation (pixels)\n",
        "\n",
        "        # Combined rotation matrix\n",
        "        M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n",
        "        # affine image\n",
        "        if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n",
        "            if self.perspective:\n",
        "                img = cv2.warpPerspective(img, M, dsize=self.size, borderValue=(114, 114, 114))\n",
        "            else:  # affine\n",
        "                img = cv2.warpAffine(img, M[:2], dsize=self.size, borderValue=(114, 114, 114))\n",
        "        return img, M, s\n",
        "\n",
        "    def apply_bboxes(self, bboxes, M):\n",
        "        \"\"\"apply affine to bboxes only.\n",
        "        Args:\n",
        "            bboxes(ndarray): list of bboxes, xyxy format, with shape (num_bboxes, 4).\n",
        "            M(ndarray): affine matrix.\n",
        "        Returns:\n",
        "            new_bboxes(ndarray): bboxes after affine, [num_bboxes, 4].\n",
        "        \"\"\"\n",
        "        n = len(bboxes)\n",
        "        if n == 0:\n",
        "            return bboxes\n",
        "\n",
        "        xy = np.ones((n * 4, 3))\n",
        "        xy[:, :2] = bboxes[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
        "        xy = xy @ M.T  # transform\n",
        "        xy = (xy[:, :2] / xy[:, 2:3] if self.perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n",
        "\n",
        "        # create new boxes\n",
        "        x = xy[:, [0, 2, 4, 6]]\n",
        "        y = xy[:, [1, 3, 5, 7]]\n",
        "        return np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "\n",
        "    def apply_segments(self, segments, M):\n",
        "        \"\"\"apply affine to segments and generate new bboxes from segments.\n",
        "        Args:\n",
        "            segments(ndarray): list of segments, [num_samples, 500, 2].\n",
        "            M(ndarray): affine matrix.\n",
        "        Returns:\n",
        "            new_segments(ndarray): list of segments after affine, [num_samples, 500, 2].\n",
        "            new_bboxes(ndarray): bboxes after affine, [N, 4].\n",
        "        \"\"\"\n",
        "        n, num = segments.shape[:2]\n",
        "        if n == 0:\n",
        "            return [], segments\n",
        "\n",
        "        xy = np.ones((n * num, 3))\n",
        "        segments = segments.reshape(-1, 2)\n",
        "        xy[:, :2] = segments\n",
        "        xy = xy @ M.T  # transform\n",
        "        xy = xy[:, :2] / xy[:, 2:3]\n",
        "        segments = xy.reshape(n, -1, 2)\n",
        "        bboxes = np.stack([segment2box(xy, self.size[0], self.size[1]) for xy in segments], 0)\n",
        "        return bboxes, segments\n",
        "\n",
        "    def apply_keypoints(self, keypoints, M):\n",
        "        \"\"\"apply affine to keypoints.\n",
        "        Args:\n",
        "            keypoints(ndarray): keypoints, [N, 17, 2].\n",
        "            M(ndarray): affine matrix.\n",
        "        Return:\n",
        "            new_keypoints(ndarray): keypoints after affine, [N, 17, 2].\n",
        "        \"\"\"\n",
        "        n = len(keypoints)\n",
        "        if n == 0:\n",
        "            return keypoints\n",
        "        new_keypoints = np.ones((n * 17, 3))\n",
        "        new_keypoints[:, :2] = keypoints.reshape(n * 17, 2)  # num_kpt is hardcoded to 17\n",
        "        new_keypoints = new_keypoints @ M.T  # transform\n",
        "        new_keypoints = (new_keypoints[:, :2] / new_keypoints[:, 2:3]).reshape(n, 34)  # perspective rescale or affine\n",
        "        new_keypoints[keypoints.reshape(-1, 34) == 0] = 0\n",
        "        x_kpts = new_keypoints[:, list(range(0, 34, 2))]\n",
        "        y_kpts = new_keypoints[:, list(range(1, 34, 2))]\n",
        "\n",
        "        x_kpts[np.logical_or.reduce((x_kpts < 0, x_kpts > self.size[0], y_kpts < 0, y_kpts > self.size[1]))] = 0\n",
        "        y_kpts[np.logical_or.reduce((x_kpts < 0, x_kpts > self.size[0], y_kpts < 0, y_kpts > self.size[1]))] = 0\n",
        "        new_keypoints[:, list(range(0, 34, 2))] = x_kpts\n",
        "        new_keypoints[:, list(range(1, 34, 2))] = y_kpts\n",
        "        return new_keypoints.reshape(n, 17, 2)\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        \"\"\"\n",
        "        Affine images and targets.\n",
        "        Args:\n",
        "            labels(Dict): a dict of `bboxes`, `segments`, `keypoints`.\n",
        "        \"\"\"\n",
        "        if self.pre_transform and \"mosaic_border\" not in labels:\n",
        "            labels = self.pre_transform(labels)\n",
        "            labels.pop(\"ratio_pad\")  # do not need ratio pad\n",
        "\n",
        "        img = labels[\"img\"]\n",
        "        cls = labels[\"cls\"]\n",
        "        instances = labels.pop(\"instances\")\n",
        "        # make sure the coord formats are right\n",
        "        instances.convert_bbox(format=\"xyxy\")\n",
        "        instances.denormalize(*img.shape[:2][::-1])\n",
        "\n",
        "        border = labels.pop(\"mosaic_border\", self.border)\n",
        "        self.size = img.shape[1] + border[1] * 2, img.shape[0] + border[0] * 2  # w, h\n",
        "        # M is affine matrix\n",
        "        # scale for func:`box_candidates`\n",
        "        img, M, scale = self.affine_transform(img, border)\n",
        "\n",
        "        bboxes = self.apply_bboxes(instances.bboxes, M)\n",
        "\n",
        "        segments = instances.segments\n",
        "        keypoints = instances.keypoints\n",
        "        # update bboxes if there are segments.\n",
        "        if len(segments):\n",
        "            bboxes, segments = self.apply_segments(segments, M)\n",
        "\n",
        "        if keypoints is not None:\n",
        "            keypoints = self.apply_keypoints(keypoints, M)\n",
        "        new_instances = Instances(bboxes, segments, keypoints, bbox_format=\"xyxy\", normalized=False)\n",
        "        # clip\n",
        "        new_instances.clip(*self.size)\n",
        "\n",
        "        # filter instances\n",
        "        instances.scale(scale_w=scale, scale_h=scale, bbox_only=True)\n",
        "        # make the bboxes have the same scale with new_bboxes\n",
        "        i = self.box_candidates(box1=instances.bboxes.T,\n",
        "                                box2=new_instances.bboxes.T,\n",
        "                                area_thr=0.01 if len(segments) else 0.10)\n",
        "        labels[\"instances\"] = new_instances[i]\n",
        "        labels[\"cls\"] = cls[i]\n",
        "        labels[\"img\"] = img\n",
        "        labels[\"resized_shape\"] = img.shape[:2]\n",
        "        return labels\n",
        "\n",
        "    def box_candidates(self, box1, box2, wh_thr=2, ar_thr=100, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n",
        "        # Compute box candidates: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n",
        "        w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
        "        w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
        "        ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\n",
        "        return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates\n",
        "\n",
        "\n",
        "class RandomHSV:\n",
        "\n",
        "    def __init__(self, hgain=0.5, sgain=0.5, vgain=0.5) -> None:\n",
        "        self.hgain = hgain\n",
        "        self.sgain = sgain\n",
        "        self.vgain = vgain\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        img = labels[\"img\"]\n",
        "        if self.hgain or self.sgain or self.vgain:\n",
        "            r = np.random.uniform(-1, 1, 3) * [self.hgain, self.sgain, self.vgain] + 1  # random gains\n",
        "            hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
        "            dtype = img.dtype  # uint8\n",
        "\n",
        "            x = np.arange(0, 256, dtype=r.dtype)\n",
        "            lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
        "            lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
        "            lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
        "\n",
        "            im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n",
        "            cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\n",
        "        return labels\n",
        "\n",
        "\n",
        "class RandomFlip:\n",
        "\n",
        "    def __init__(self, p=0.5, direction=\"horizontal\") -> None:\n",
        "        assert direction in [\"horizontal\", \"vertical\"], f\"Support direction `horizontal` or `vertical`, got {direction}\"\n",
        "        assert 0 <= p <= 1.0\n",
        "\n",
        "        self.p = p\n",
        "        self.direction = direction\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        img = labels[\"img\"]\n",
        "        instances = labels.pop(\"instances\")\n",
        "        instances.convert_bbox(format=\"xywh\")\n",
        "        h, w = img.shape[:2]\n",
        "        h = 1 if instances.normalized else h\n",
        "        w = 1 if instances.normalized else w\n",
        "\n",
        "        # Flip up-down\n",
        "        if self.direction == \"vertical\" and random.random() < self.p:\n",
        "            img = np.flipud(img)\n",
        "            instances.flipud(h)\n",
        "        if self.direction == \"horizontal\" and random.random() < self.p:\n",
        "            img = np.fliplr(img)\n",
        "            instances.fliplr(w)\n",
        "        labels[\"img\"] = np.ascontiguousarray(img)\n",
        "        labels[\"instances\"] = instances\n",
        "        return labels\n",
        "\n",
        "\n",
        "class LetterBox:\n",
        "    \"\"\"Resize image and padding for detection, instance segmentation, pose\"\"\"\n",
        "\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]  # current shape [height, width]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        # Scale ratio (new / old)\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        # Compute padding\n",
        "        ratio = r, r  # width, height ratios\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:  # minimum rectangle\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:  # stretch\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        dw /= 2  # divide padding into 2 sides\n",
        "        dh /= 2\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (dw, dh))  # for evaluation\n",
        "\n",
        "        if shape[::-1] != new_unpad:  # resize\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
        "                                 value=(114, 114, 114))  # add border\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        \"\"\"Update labels\"\"\"\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "\n",
        "class CopyPaste:\n",
        "\n",
        "    def __init__(self, p=0.5) -> None:\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\n",
        "        im = labels[\"img\"]\n",
        "        cls = labels[\"cls\"]\n",
        "        h, w = im.shape[:2]\n",
        "        instances = labels.pop(\"instances\")\n",
        "        instances.convert_bbox(format=\"xyxy\")\n",
        "        instances.denormalize(w, h)\n",
        "        if self.p and len(instances.segments):\n",
        "            n = len(instances)\n",
        "            _, w, _ = im.shape  # height, width, channels\n",
        "            im_new = np.zeros(im.shape, np.uint8)\n",
        "\n",
        "            # calculate ioa first then select indexes randomly\n",
        "            ins_flip = deepcopy(instances)\n",
        "            ins_flip.fliplr(w)\n",
        "\n",
        "            ioa = bbox_ioa(ins_flip.bboxes, instances.bboxes)  # intersection over area, (N, M)\n",
        "            indexes = np.nonzero((ioa < 0.30).all(1))[0]  # (N, )\n",
        "            n = len(indexes)\n",
        "            for j in random.sample(list(indexes), k=round(self.p * n)):\n",
        "                cls = np.concatenate((cls, cls[[j]]), axis=0)\n",
        "                instances = Instances.concatenate((instances, ins_flip[[j]]), axis=0)\n",
        "                cv2.drawContours(im_new, instances.segments[[j]].astype(np.int32), -1, (1, 1, 1), cv2.FILLED)\n",
        "\n",
        "            result = cv2.flip(im, 1)  # augment segments (flip left-right)\n",
        "            i = cv2.flip(im_new, 1).astype(bool)\n",
        "            im[i] = result[i]  # cv2.imwrite('debug.jpg', im)  # debug\n",
        "\n",
        "        labels[\"img\"] = im\n",
        "        labels[\"cls\"] = cls\n",
        "        labels[\"instances\"] = instances\n",
        "        return labels\n",
        "\n",
        "\n",
        "class Albumentations:\n",
        "    # YOLOv8 Albumentations class (optional, only used if package is installed)\n",
        "    def __init__(self, p=1.0):\n",
        "        self.p = p\n",
        "        self.transform = None\n",
        "        prefix = colorstr(\"albumentations: \")\n",
        "        try:\n",
        "            import albumentations as A\n",
        "\n",
        "            check_version(A.__version__, \"1.0.3\", hard=True)  # version requirement\n",
        "\n",
        "\n",
        "\n",
        "############################################# Augmentation Parameters ###############################################\n",
        "            T = [\n",
        "                A.Blur(p=0.01),#0.01),\n",
        "                A.MedianBlur(p=0.2),#0.01\n",
        "                A.ToGray(p=0.1),\n",
        "                A.CLAHE(p=0.2),\n",
        "                A.RandomBrightnessContrast(p=0.2),#0.0),\n",
        "                A.RandomGamma(p=0.2),\n",
        "                A.GaussNoise(p=0.2),\n",
        "                A.Sharpen(p=0.2),\n",
        "                A.geometric.rotate.SafeRotate(p=0.4),\n",
        "                A.ImageCompression(quality_lower=75, p=0.0),]  # transforms\n",
        "            self.transform = A.Compose(T, bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]))\n",
        "\n",
        "############################################# Augmentation Parameters ###############################################\n",
        "\n",
        "\n",
        "\n",
        "            LOGGER.info(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p))\n",
        "        except ImportError:  # package not installed, skip\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            LOGGER.info(f\"{prefix}{e}\")\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        im = labels[\"img\"]\n",
        "        cls = labels[\"cls\"]\n",
        "        if len(cls):\n",
        "            labels[\"instances\"].convert_bbox(\"xywh\")\n",
        "            labels[\"instances\"].normalize(*im.shape[:2][::-1])\n",
        "            bboxes = labels[\"instances\"].bboxes\n",
        "            # TODO: add supports of segments and keypoints\n",
        "            if self.transform and random.random() < self.p:\n",
        "                new = self.transform(image=im, bboxes=bboxes, class_labels=cls)  # transformed\n",
        "                labels[\"img\"] = new[\"image\"]\n",
        "                labels[\"cls\"] = np.array(new[\"class_labels\"])\n",
        "            labels[\"instances\"].update(bboxes=bboxes)\n",
        "        return labels\n",
        "\n",
        "\n",
        "# TODO: technically this is not an augmentation, maybe we should put this to another files\n",
        "class Format:\n",
        "\n",
        "    def __init__(self,\n",
        "                 bbox_format=\"xywh\",\n",
        "                 normalize=True,\n",
        "                 return_mask=False,\n",
        "                 return_keypoint=False,\n",
        "                 mask_ratio=4,\n",
        "                 mask_overlap=True,\n",
        "                 batch_idx=True):\n",
        "        self.bbox_format = bbox_format\n",
        "        self.normalize = normalize\n",
        "        self.return_mask = return_mask  # set False when training detection only\n",
        "        self.return_keypoint = return_keypoint\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.mask_overlap = mask_overlap\n",
        "        self.batch_idx = batch_idx  # keep the batch indexes\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        img = labels.pop(\"img\")\n",
        "        h, w = img.shape[:2]\n",
        "        cls = labels.pop(\"cls\")\n",
        "        instances = labels.pop(\"instances\")\n",
        "        instances.convert_bbox(format=self.bbox_format)\n",
        "        instances.denormalize(w, h)\n",
        "        nl = len(instances)\n",
        "\n",
        "        if self.return_mask:\n",
        "            if nl:\n",
        "                masks, instances, cls = self._format_segments(instances, cls, w, h)\n",
        "                masks = torch.from_numpy(masks)\n",
        "            else:\n",
        "                masks = torch.zeros(1 if self.mask_overlap else nl, img.shape[0] // self.mask_ratio,\n",
        "                                    img.shape[1] // self.mask_ratio)\n",
        "            labels[\"masks\"] = masks\n",
        "        if self.normalize:\n",
        "            instances.normalize(w, h)\n",
        "        labels[\"img\"] = self._format_img(img)\n",
        "        labels[\"cls\"] = torch.from_numpy(cls) if nl else torch.zeros(nl)\n",
        "        labels[\"bboxes\"] = torch.from_numpy(instances.bboxes) if nl else torch.zeros((nl, 4))\n",
        "        if self.return_keypoint:\n",
        "            labels[\"keypoints\"] = torch.from_numpy(instances.keypoints) if nl else torch.zeros((nl, 17, 2))\n",
        "        # then we can use collate_fn\n",
        "        if self.batch_idx:\n",
        "            labels[\"batch_idx\"] = torch.zeros(nl)\n",
        "        return labels\n",
        "\n",
        "    def _format_img(self, img):\n",
        "        if len(img.shape) < 3:\n",
        "            img = np.expand_dims(img, -1)\n",
        "        img = np.ascontiguousarray(img.transpose(2, 0, 1)[::-1])\n",
        "        img = torch.from_numpy(img)\n",
        "        return img\n",
        "\n",
        "    def _format_segments(self, instances, cls, w, h):\n",
        "        \"\"\"convert polygon points to bitmap\"\"\"\n",
        "        segments = instances.segments\n",
        "        if self.mask_overlap:\n",
        "            masks, sorted_idx = polygons2masks_overlap((h, w), segments, downsample_ratio=self.mask_ratio)\n",
        "            masks = masks[None]  # (640, 640) -> (1, 640, 640)\n",
        "            instances = instances[sorted_idx]\n",
        "            cls = cls[sorted_idx]\n",
        "        else:\n",
        "            masks = polygons2masks((h, w), segments, color=1, downsample_ratio=self.mask_ratio)\n",
        "\n",
        "        return masks, instances, cls\n",
        "\n",
        "\n",
        "def v8_transforms(dataset, imgsz, hyp):\n",
        "    pre_transform = Compose([\n",
        "        Mosaic(dataset, imgsz=imgsz, p=hyp.mosaic, border=[-imgsz // 2, -imgsz // 2]),\n",
        "        CopyPaste(p=hyp.copy_paste),\n",
        "        RandomPerspective(\n",
        "            degrees=hyp.degrees,\n",
        "            translate=hyp.translate,\n",
        "            scale=hyp.scale,\n",
        "            shear=hyp.shear,\n",
        "            perspective=hyp.perspective,\n",
        "            pre_transform=LetterBox(new_shape=(imgsz, imgsz)),\n",
        "        ),])\n",
        "    return Compose([\n",
        "        pre_transform,\n",
        "        MixUp(dataset, pre_transform=pre_transform, p=hyp.mixup),\n",
        "        Albumentations(p=1.0),\n",
        "        RandomHSV(hgain=hyp.hsv_h, sgain=hyp.hsv_s, vgain=hyp.hsv_v),\n",
        "        RandomFlip(direction=\"vertical\", p=hyp.flipud),\n",
        "        RandomFlip(direction=\"horizontal\", p=hyp.fliplr),])  # transforms\n",
        "\n",
        "\n",
        "# Classification augmentations -----------------------------------------------------------------------------------------\n",
        "def classify_transforms(size=224):\n",
        "    # Transforms to apply if albumentations not installed\n",
        "    assert isinstance(size, int), f\"ERROR: classify_transforms size {size} must be integer, not (list, tuple)\"\n",
        "    # T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])\n",
        "    return T.Compose([CenterCrop(size), ToTensor(), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])\n",
        "\n",
        "\n",
        "def classify_albumentations(\n",
        "        augment=True,\n",
        "        size=224,\n",
        "        scale=(0.08, 1.0),\n",
        "        hflip=0.5,\n",
        "        vflip=0.0,\n",
        "        jitter=0.4,\n",
        "        mean=IMAGENET_MEAN,\n",
        "        std=IMAGENET_STD,\n",
        "        auto_aug=False,\n",
        "):\n",
        "    # YOLOv8 classification Albumentations (optional, only used if package is installed)\n",
        "    prefix = colorstr(\"albumentations: \")\n",
        "    try:\n",
        "        import albumentations as A\n",
        "        from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "        check_version(A.__version__, \"1.0.3\", hard=True)  # version requirement\n",
        "        if augment:  # Resize and crop\n",
        "            T = [A.RandomResizedCrop(height=size, width=size, scale=scale)]\n",
        "            if auto_aug:\n",
        "                # TODO: implement AugMix, AutoAug & RandAug in albumentation\n",
        "                LOGGER.info(f\"{prefix}auto augmentations are currently not supported\")\n",
        "            else:\n",
        "                if hflip > 0:\n",
        "                    T += [A.HorizontalFlip(p=hflip)]\n",
        "                if vflip > 0:\n",
        "                    T += [A.VerticalFlip(p=vflip)]\n",
        "                if jitter > 0:\n",
        "                    color_jitter = (float(jitter),) * 3  # repeat value for brightness, contrast, saturation, 0 hue\n",
        "                    T += [A.ColorJitter(*color_jitter, 0)]\n",
        "        else:  # Use fixed crop for eval set (reproducibility)\n",
        "            T = [A.SmallestMaxSize(max_size=size), A.CenterCrop(height=size, width=size)]\n",
        "        T += [A.Normalize(mean=mean, std=std), ToTensorV2()]  # Normalize and convert to Tensor\n",
        "        LOGGER.info(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p))\n",
        "        return A.Compose(T)\n",
        "\n",
        "    except ImportError:  # package not installed, skip\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        LOGGER.info(f\"{prefix}{e}\")\n",
        "\n",
        "\n",
        "class ClassifyLetterBox:\n",
        "    # YOLOv8 LetterBox class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])\n",
        "    def __init__(self, size=(640, 640), auto=False, stride=32):\n",
        "        super().__init__()\n",
        "        self.h, self.w = (size, size) if isinstance(size, int) else size\n",
        "        self.auto = auto  # pass max size integer, automatically solve for short side using stride\n",
        "        self.stride = stride  # used with auto\n",
        "\n",
        "    def __call__(self, im):  # im = np.array HWC\n",
        "        imh, imw = im.shape[:2]\n",
        "        r = min(self.h / imh, self.w / imw)  # ratio of new/old\n",
        "        h, w = round(imh * r), round(imw * r)  # resized image\n",
        "        hs, ws = (math.ceil(x / self.stride) * self.stride for x in (h, w)) if self.auto else self.h, self.w\n",
        "        top, left = round((hs - h) / 2 - 0.1), round((ws - w) / 2 - 0.1)\n",
        "        im_out = np.full((self.h, self.w, 3), 114, dtype=im.dtype)\n",
        "        im_out[top:top + h, left:left + w] = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "        return im_out\n",
        "\n",
        "\n",
        "class CenterCrop:\n",
        "    # YOLOv8 CenterCrop class for image preprocessing, i.e. T.Compose([CenterCrop(size), ToTensor()])\n",
        "    def __init__(self, size=640):\n",
        "        super().__init__()\n",
        "        self.h, self.w = (size, size) if isinstance(size, int) else size\n",
        "\n",
        "    def __call__(self, im):  # im = np.array HWC\n",
        "        imh, imw = im.shape[:2]\n",
        "        m = min(imh, imw)  # min dimension\n",
        "        top, left = (imh - m) // 2, (imw - m) // 2\n",
        "        return cv2.resize(im[top:top + m, left:left + m], (self.w, self.h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "\n",
        "class ToTensor:\n",
        "    # YOLOv8 ToTensor class for image preprocessing, i.e. T.Compose([LetterBox(size), ToTensor()])\n",
        "    def __init__(self, half=False):\n",
        "        super().__init__()\n",
        "        self.half = half\n",
        "\n",
        "    def __call__(self, im):  # im = np.array HWC in BGR order\n",
        "        im = np.ascontiguousarray(im.transpose((2, 0, 1))[::-1])  # HWC to CHW -> BGR to RGB -> contiguous\n",
        "        im = torch.from_numpy(im)  # to torch\n",
        "        im = im.half() if self.half else im.float()  # uint8 to fp16/32\n",
        "        im /= 255.0  # 0-255 to 0.0-1.0\n",
        "        return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FswcPjRjW_nj",
        "outputId": "8a9e7796-ac48-40db-e598-c877e6b5712d"
      },
      "outputs": [],
      "source": [
        "# If you want follow and save training status on ClearML, run code block that about ClearML.\n",
        "!pip install clearml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aUJUBm0XIPB",
        "outputId": "16a1a118-0434-4813-8e6a-47afaac01600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=Q8TH9VSFP31QQM0QIJW7\n",
            "env: CLEARML_API_SECRET_KEY=sM1oWQq7slP6D9L3TRKvXQfxmmOvlKoKaI2N87h01AbiGkz46P\n"
          ]
        }
      ],
      "source": [
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY= \"enter private key\"\n",
        "%env CLEARML_API_SECRET_KEY="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCHMlJQsXclB",
        "outputId": "8c904dd0-17c5-477f-9371-93f44b921008"
      },
      "outputs": [],
      "source": [
        "from clearml import Task\n",
        "task = Task.init(project_name=\"my project\", task_name=\"yolov8n-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6lXn6Dm7nfH"
      },
      "outputs": [],
      "source": [
        "# This stop the active task of ClearML.\n",
        "task.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "6caca719-2ea7-4df9-d9f9-dd44459c4746"
      },
      "outputs": [],
      "source": [
        "# Train YOLOv8, It use hyperparameter .yaml file with cfg argument. You should modify deafult.yaml above or \n",
        "# create new .yaml file before this step.\n",
        "\n",
        "%cd {MAIN_DIRECTORY}/ultralytics\n",
        "!yolo train cfg = {MAIN_DIRECTORY}/ultralytics/ultralytics/yolo/cfg/default.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnZbai7qpIck",
        "outputId": "f406dde4-565c-42e1-c58f-d42b4040b866"
      },
      "outputs": [],
      "source": [
        "!yolo help"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZZeNrLCQG6"
      },
      "source": [
        "# 2. Export Model File as ONNX or TensorRT\n",
        "\n",
        "\n",
        "| Format                                                                     | `format=`          | Model                     |\n",
        "|----------------------------------------------------------------------------|--------------------|---------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                            | -                  | `yolov8n.pt`              |\n",
        "| [TorchScript](https://pytorch.org/docs/stable/jit.html)                    | `torchscript`      | `yolov8n.torchscript`     |\n",
        "| [ONNX](https://onnx.ai/)                                                   | `onnx`             | `yolov8n.onnx`            |\n",
        "| [OpenVINO](https://docs.openvino.ai/latest/index.html)                     | `openvino`         | `yolov8n_openvino_model/` |\n",
        "| [TensorRT](https://developer.nvidia.com/tensorrt)                          | `engine`           | `yolov8n.engine`          |\n",
        "| [CoreML](https://github.com/apple/coremltools)                             | `coreml`           | `yolov8n.mlmodel`         |\n",
        "| [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`      | `yolov8n_saved_model/`    |\n",
        "| [TensorFlow GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`               | `yolov8n.pb`              |\n",
        "| [TensorFlow Lite](https://www.tensorflow.org/lite)                         | `tflite`           | `yolov8n.tflite`          |\n",
        "| [TensorFlow Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`          | `yolov8n_edgetpu.tflite`  |\n",
        "| [TensorFlow.js](https://www.tensorflow.org/js)                             | `tfjs`             | `yolov8n_web_model/`      |\n",
        "| [PaddlePaddle](https://github.com/PaddlePaddle)                            | `paddle`           | `yolov8n_paddle_model/`   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIjW4igCjqD",
        "outputId": "69cab2fb-cbfa-4acf-8e29-9c4fb6f4a38f"
      },
      "outputs": [],
      "source": [
        "# You can export the model file resulting from the training and convert it to the desired format.\n",
        "MODEL_PATH = 'yolov8n.pt'\n",
        "\n",
        "!yolo export model=MODEL_PATH = 'yolov8n.pt' format=onnx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qhl68MQma9xe"
      },
      "source": [
        "# 3. Tracking\n",
        "\n",
        "ByteTracker will be used from yolov8_tracking repository. If you want, use another tracking module where in this repo. Because paid attention speed factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM9sNaGIbDMX",
        "outputId": "5d906b9d-454d-4950-f254-6558cf7e0b85"
      },
      "outputs": [],
      "source": [
        "%cd {MAIN_DIRECTORY}\n",
        "!git clone --recurse-submodules https://github.com/mikel-brostrom/yolov8_tracking  # clone repo\n",
        "%cd {MAIN_DIRECTORY}/yolov8_tracking\n",
        "\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "%pip install thop                  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWK3p_xwbTcN",
        "outputId": "32a632e6-e13c-4087-da76-92e4a8b3e404"
      },
      "outputs": [],
      "source": [
        "%cd {MAIN_DIRECTORY}/yolov8_tracking\n",
        "!python track.py --yolo-weights {MAIN_DIRECTORY}/ultralytics/runs/detect/train9/weights/best.onnx --tracking-method bytetrack --source {MAIN_DIRECTORY}/test.mp4 --save-vid --save-txt --show-vid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "def show_video(video_path, video_width = 600):\n",
        "   \n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        " \n",
        "show_video('./test.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4JnkELT0cIJg",
        "0eq1SMWl6Sfn"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "yolov5-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c098b26033e15e8a1ab799ac71236cca6ed4d51cab166314c497ab795a74e8c6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
